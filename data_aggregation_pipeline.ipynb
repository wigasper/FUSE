{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Aggregation and Preprocessing Pipeline\n",
    "\n",
    "The general premise of this project is that I am attempting to use citation networks to suggest MeSH terms that can be applied to PubMed citations. For example, if an article indexed on PubMed has been annotated with the MeSH term \"Neoadjuvant Therapy\", it is quite possible or even likely that some or many of its references have also been annotated with that \"Neoadjuvant Therapy\".\n",
    "\n",
    "During the initial stages of the project, I chose to use NCBI APIs to retrieve the necessary documents. Later on, I encorporate the entire PubMed and PMC corpuses.\n",
    "\n",
    "I used the 2013 MTI ML dataset as a starting point. Building citation networks requires full texts, so I was not able to attempt term prediction for every document in the dataset. Thus, my approach to the initial data aggregation and preprocessing is as follows:\n",
    "\n",
    "1. Subset 2013 MTI ML dataset for only those PMIDs that are also in PMC's Open Access subset\n",
    "2. Pull full text XMLs from PMC for articles in step 1\n",
    "3. Extract the references from full texts\n",
    "4. For each extracted reference, pull PubMed citations in XML format (which contain MeSH term annotations) from PubMed using the Pubmed API\n",
    "5. Extract the MeSH terms from each of the PubMed citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset 2013 MTI ML dataset and pull full texts from PMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import xmltodict\n",
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(\"pmc_api_pull.log\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "with open(\"./ncbi.key\") as handle:\n",
    "    api_key = handle.read()\n",
    "\n",
    "# Add source for oa_file_list here\n",
    "oa_list = pd.read_csv(\"./data/oa_file_list.csv\")\n",
    "\n",
    "# Subset the 2013 MTI dataset for only those PMIDs that\n",
    "# are also in the PMC Open Access file list\n",
    "# I really do not like that I am using Pandas here, will refactor later when\n",
    "# there is time to eliminate this dependency and slightly increase performance\n",
    "with open(\"./data/PMIDs_train\", \"r\") as fp:\n",
    "    mti_train = fp.readlines()\n",
    "    mti_train = pd.DataFrame({'PMID':mti_train})\n",
    "\n",
    "with open(\"./data/PMIDs_test\", \"r\") as fp:\n",
    "    mti_test = fp.readlines()\n",
    "    mti_test = pd.DataFrame({'PMID':mti_test})\n",
    "\n",
    "mti_subset_train = oa_list[(oa_list.PMID.isin(mti_train.PMID))]\n",
    "mti_subset_train.to_csv(\"./data/2013_MTI_in_OA_train.csv\")\n",
    "\n",
    "mti_subset_test = oa_list[(oa_list.PMID.isin(mti_test.PMID))]\n",
    "mti_subset_test.to_csv(\"./data/2013_MTI_in_OA_test.csv\")\n",
    "\n",
    "ids_to_get = mti_subset_train[\"Accession ID\"].tolist() + mti_subset_test[\"Accession ID\"].tolist()\n",
    "\n",
    "# Save full texts for each PMC ID\n",
    "for pmcid in tqdm_notebook(ids_to_get):\n",
    "    start_time = time.perf_counter()\n",
    "    file = Path(f\"./pmc_xmls/{pmcid}.xml\")\n",
    "    if not file.exists():\n",
    "        Entrez.email = \"kgasper@unomaha.edu\"\n",
    "        Entrez.api_key = api_key\n",
    "        handle = Entrez.efetch(db=\"pmc\", id=pmcid, retmode=\"xml\")\n",
    "        xmlString = handle.read()\n",
    "        element = xmltodict.parse(xmlString)\n",
    "    \n",
    "        pmc_error = False\n",
    "    \n",
    "        # Check for an error on PMC's side and record it\n",
    "        for key in element['pmc-articleset'].keys():\n",
    "            if key == 'error':\n",
    "                logger.error(f\"PMC API error - ID: {pmcid}\")\n",
    "                pmc_error = True\n",
    "    \n",
    "        if not pmc_error:\n",
    "            with open(file, \"w\") as file_out:\n",
    "                file_out.write(xmlString)\n",
    "        \n",
    "        # This is a delay in accordance with PubMed API usage guidelines\n",
    "        # PubMed allows 3 requests/sec without API key or 10 req/sec with\n",
    "        if time.perf_counter() - start_time < .33:\n",
    "            time.sleep(.33 - (time.perf_counter() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract references from full texts\n",
    "\n",
    "The primary goal here is to extract identifiers (DOI or PMID) for each reference in each article and use these identifiers to created an edge list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessions = []\n",
    "\n",
    "# You might notice the \"nohead\" in the filename, I removed the header for this file\n",
    "# using tail\n",
    "with open(\"./data/2013_MTI_in_OA_train_nohead.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.split(\",\")\n",
    "        accessions.append(line[3])\n",
    "\n",
    "with open(\"./data/2013_MTI_in_OA_test_nohead.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.split(\",\")\n",
    "        accessions.append(line[3])\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(\"reference_extraction.log\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# List for the references\n",
    "mti_refs = []\n",
    "\n",
    "# Extract references from the XML files\n",
    "for ID in tqdm_notebook(accessions):\n",
    "    try:\n",
    "        with open(f\"./pmc_xmls/{ID}.xml\", \"r\") as handle:\n",
    "            soup = BeautifulSoup(handle.read())\n",
    "            \n",
    "            sample = [ID]\n",
    "            \n",
    "            # add IDs to the error log if they don't have the 'back' tag and to \n",
    "            # the samples list if they do\n",
    "            if soup.back is None:\n",
    "                logger.error(f\"No refs: {ID}\")\n",
    "            elif soup.back is not None:\n",
    "                for pubid in soup.back.find_all('pub-id'):\n",
    "                    sample.append(pubid.string)\n",
    "                mti_refs.append(sample)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"FNFE: {ID}\")\n",
    "\n",
    "# Create dicts for ID conversions\n",
    "# Here I want to convert my PMCIDs back to PMIDs and convert any DOIs to\n",
    "# PMIDs whenever possible. The DOI -> PMID thing is not extremely important,\n",
    "# because in general PMC is good about adding PMIDs to articles' references \n",
    "# as they become available, but I wanted to be thorough \n",
    "dois = {}\n",
    "pmcids = {}\n",
    "with open(\"./data/PMC-ids-nohead.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.split(\",\")\n",
    "        if len(line) > 9:\n",
    "            if line[7]:\n",
    "                dois[line[7]] = line[9]\n",
    "            pmcids[line[8]] = line[9]\n",
    "\n",
    "# This function converts a DOI or PMCID to a PMID\n",
    "def fetch_pmid(identifier, dois, pmcids, logger):\n",
    "    pmid = \"\"\n",
    "    if re.match(\"^10\\..*$\", identifier):\n",
    "        if identifier in dois.keys():\n",
    "            pmid = dois[identifier]\n",
    "        return pmid if pmid else np.NaN\n",
    "\n",
    "    if re.match(\"^PMC.*$\", identifier) and identifier in pmcids.keys():\n",
    "        pmid = pmcids[identifier]\n",
    "        if pmid:\n",
    "            return pmid\n",
    "        else:\n",
    "            logger.error(f\"PMCID conversion error: {identifier}\")\n",
    "            return identifier\n",
    "    \n",
    "    # Return original identifier if not a DOI or PMCID\n",
    "    return identifier\n",
    "\n",
    "# Convert IDs to PMIDs if possible\n",
    "for sample in mti_refs:\n",
    "    for index in range(len(sample)):\n",
    "        sample[index] = fetch_pmid(sample[index], dois, pmcids, logger)\n",
    "\n",
    "edge_list = []\n",
    "\n",
    "# Convert to edge list format and drop non-PMID identifiers:\n",
    "for sample in mti_refs:\n",
    "    for index in range(1, len(sample)):\n",
    "        if sample[index] is not np.NaN and re.match(\"^\\d*$\", sample[index]):\n",
    "            edge_list.append((sample[0], str(sample[index])))\n",
    "\n",
    "# Remove duplicates:\n",
    "edge_list = list(set(edge_list))\n",
    "edge_list.sort()\n",
    "\n",
    "# Write output\n",
    "with open(\"./data/edge_list.csv\", \"w\") as out:\n",
    "    for edge in edge_list:\n",
    "        out.write(\"\".join([edge[0], \",\", edge[1], \"\\n\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull PubMed citations in XML format\n",
    "\n",
    "APIs are really not ideal for data collection like this, but because I only need ~1-2% of the documents in PubMed's corpus, I chose to request documents using the API. It's also advantageous to save each document individually (the corpus data dumps are not collections of individual documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(\"pubmed_api_pull.log\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "with open(\"./ncbi.key\") as handle:\n",
    "    api_key = handle.read()\n",
    "\n",
    "ids_to_get = []\n",
    "\n",
    "with open(\"./data/edge_list.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\",\")\n",
    "        ids_to_get.append(line[0])\n",
    "        ids_to_get.append(line[1])\n",
    "\n",
    "# Drop duplicates:\n",
    "ids_to_get = list(dict.fromkeys(ids_to_get))\n",
    "\n",
    "for pmid in ids_to_get:\n",
    "    start_time = time.perf_counter()\n",
    "    file = Path(f\"./mesh_xmls/{pmid}.xml\")\n",
    "\n",
    "    if not file.exists():\n",
    "        Entrez.email = \"kgasper@unomaha.edu\"\n",
    "        Entrez.api_key = api_key\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pmid, retmode=\"xml\")\n",
    "        xmlString = handle.read()\n",
    "        element = xmltodict.parse(xmlString)\n",
    "    \n",
    "        pm_error = False\n",
    "    \n",
    "        # Check for an error on PubMed's side and record it\n",
    "        if isinstance(element['PubmedArticleSet'], dict):\n",
    "            for key in element['PubmedArticleSet'].keys():\n",
    "                if key == 'error':\n",
    "                    logger.error(f\"PubMed API - ID: {pmid}\")\n",
    "                    pm_error = True\n",
    "            if not pm_error:\n",
    "                with open(f\"../mesh_xmls/{pmid}.xml\", \"w\") as file_out:\n",
    "                    file_out.write(xmlString)\n",
    "        if not isinstance(element['PubmedArticleSet'], dict):\n",
    "            logger.error(f\"Not dict - ID: {pmid}\")\n",
    "            \n",
    "        # This is a delay in accordance with PubMed API usage guidelines\n",
    "        if time.perf_counter() - start_time < .1:\n",
    "            time.sleep(.1 - (time.perf_counter() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract MeSH term annotations from each PubMed citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(\"mesh_term_extraction.log\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "doc_refs_dict = {}\n",
    "\n",
    "ids_to_get = []\n",
    "\n",
    "with open(\"./data/edge_list.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\",\")\n",
    "        \n",
    "        if line[0] not in doc_refs_dict.keys():\n",
    "            doc_refs_dict[line[0]] = []\n",
    "        \n",
    "        doc_refs_dict[line[0]].append(line[1])\n",
    "        \n",
    "        ids_to_get.append(line[1])\n",
    "\n",
    "# Drop duplicates\n",
    "ids_to_get = list(dict.fromkeys(ids_to_get))\n",
    "\n",
    "# Create a dict to store the MeSH terms for each PMID\n",
    "doc_term_dict = {}\n",
    "\n",
    "# Extract MeSH terms for each PMID\n",
    "for pmid in tqdm_notebook(ids_to_get):\n",
    "    try:\n",
    "        with open(f\"./mesh_xmls/{pmid}.xml\", \"r\") as handle:\n",
    "            soup = BeautifulSoup(handle.read())\n",
    "            \n",
    "            mesh_terms = []\n",
    "                            \n",
    "            for mesh_heading in soup.find_all(\"meshheading\"):\n",
    "                if mesh_heading.descriptorname is not None:\n",
    "                    term_id = mesh_heading.descriptorname['ui']\n",
    "                    mesh_terms.append(term_id)\n",
    "\n",
    "            doc_term_dict[pmid] = mesh_terms\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"FNFE: {pmid}\")\n",
    "\n",
    "# Get term counts for references of each parent node\n",
    "term_counts = []\n",
    "\n",
    "for doc in doc_refs_dict.keys():\n",
    "    try:\n",
    "        doc_counts = {}\n",
    "        for ref in doc_refs_dict[doc]:\n",
    "            for term in doc_term_dict[ref]:\n",
    "                if term not in doc_counts.keys():\n",
    "                    doc_counts[term] = 1\n",
    "                else:\n",
    "                    doc_counts[term] += 1\n",
    "                    \n",
    "        term_counts.append([doc, doc_counts])\n",
    "    except KeyError:\n",
    "        logger.error(f\"KeyError at counts - PMID: {ref}\")\n",
    "\n",
    "# Change counts to relative frequency\n",
    "for doc in term_counts:\n",
    "    total_count = 0\n",
    "    for term_id in doc[1].keys():\n",
    "        total_count += doc[1][term_id]\n",
    "    for term_id in doc[1].keys():\n",
    "        doc[1][term_id] = doc[1][term_id] / total_count\n",
    "\n",
    "# Why JSON? This is the most convenient format for the baseline model\n",
    "# A sparse matrix with 10k rows and 27k columns takes up significantly\n",
    "# more space (JSON: 83.5 MB, csv: 448.8 MB) and be more difficult to work with\n",
    "with open(\"./data/term_freqs.json\", \"w\") as out:\n",
    "    json.dump(term_counts, out)\n",
    "        \n",
    "# Get response MeSH terms to evaluate against\n",
    "response_ids = [sample[0] for sample in term_counts]\n",
    "\n",
    "# Create a dict to store the MeSH terms for each PMID\n",
    "doc_term_dict = {}\n",
    "\n",
    "# Extract MeSH terms for each PMID\n",
    "for pmid in tqdm(response_ids):\n",
    "    try:\n",
    "        with open(f\"../mesh_xmls/{pmid}.xml\", \"r\") as handle:\n",
    "            soup = BeautifulSoup(handle.read())\n",
    "            \n",
    "            mesh_terms = []\n",
    "                            \n",
    "            for mesh_heading in soup.find_all(\"meshheading\"):\n",
    "                if mesh_heading.descriptorname is not None:\n",
    "                    term_id = mesh_heading.descriptorname['ui']\n",
    "                    mesh_terms.append(term_id)\n",
    "\n",
    "            doc_term_dict[pmid] = mesh_terms\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"FNFE: {pmid}\")\n",
    "\n",
    "with open(\"./data/baseline_solution.json\", \"w\") as out:\n",
    "    json.dump(doc_term_dict, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
