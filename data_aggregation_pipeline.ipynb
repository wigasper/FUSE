{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Aggregation and Preprocessing Pipeline\n",
    "\n",
    "The general premise of this project is that I am attempting to use citation networks to suggest MeSH terms that can be applied to PubMed citations. For example, if an article indexed on PubMed has been annotated with the MeSH term \"Neoadjuvant Therapy\", it is quite possible or even likely that some or many of its references have also been annotated with that \"Neoadjuvant Therapy\".\n",
    "\n",
    "During the initial stages of the project, I chose to use NCBI APIs to retrieve the necessary documents. Later on, I encorporate the entire PubMed and PMC corpuses.\n",
    "\n",
    "I used the 2013 MTI ML dataset as a starting point. Building citation networks requires full texts, so I was not able to attempt term prediction for every document in the dataset. Thus, my approach to the initial data aggregation and preprocessing is as follows:\n",
    "\n",
    "1. Subset 2013 MTI ML dataset for only those PMIDs that are also in PMC's Open Access subset\n",
    "2. Pull full text XMLs from PMC for articles in step 1\n",
    "3. Extract the references from full texts\n",
    "4. For each extracted reference, pull PubMed citations in XML format (which contain MeSH term annotations) from PubMed using the Pubmed API\n",
    "5. Extract the MeSH terms from each of the PubMed citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset 2013 MTI ML dataset and pull full texts from PMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import xmltodict\n",
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(\"pmc_api_pull.log\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "with open(\"../ncbi.key\") as handle:\n",
    "    api_key = handle.read()\n",
    "\n",
    "# Add source for oa_file_list here\n",
    "oa_list = pd.read_csv(\"../data/oa_file_list.csv\")\n",
    "\n",
    "# Subset the 2013 MTI dataset for only those PMIDs that\n",
    "# are also in the PMC Open Access file list\n",
    "# I really do not like that I am using Pandas here, will refactor later when\n",
    "# there is time to eliminate this dependency and slightly increase performance\n",
    "with open(\"../data/PMIDs_train\", \"r\") as fp:\n",
    "    mti_train = fp.readlines()\n",
    "    mti_train = pd.DataFrame({'PMID':mti_train})\n",
    "\n",
    "with open(\"../data/PMIDs_test\", \"r\") as fp:\n",
    "    mti_test = fp.readlines()\n",
    "    mti_test = pd.DataFrame({'PMID':mti_test})\n",
    "\n",
    "mti_subset_train = oa_list[(oa_list.PMID.isin(mti_train.PMID))]\n",
    "mti_subset_train.to_csv(\"2013_MTI_in_OA_train.csv\")\n",
    "\n",
    "mti_subset_test = oa_list[(oa_list.PMID.isin(mti_test.PMID))]\n",
    "mti_subset_test.to_csv(\"2013_MTI_in_OA_test.csv\")\n",
    "\n",
    "ids_to_get = mti_subset_train[\"Accession ID\"].tolist() + mti_subset_test[\"Accession ID\"].tolist()\n",
    "\n",
    "# Save full texts for each PMC ID\n",
    "for pmcid in tqdm_notebook(ids_to_get):\n",
    "    start_time = time.perf_counter()\n",
    "    file = Path(f\"../pmc_xmls/{pmcid}.xml\")\n",
    "    if not file.exists():\n",
    "        Entrez.email = \"kgasper@unomaha.edu\"\n",
    "        Entrez.api_key = api_key\n",
    "        handle = Entrez.efetch(db=\"pmc\", id=pmcid, retmode=\"xml\")\n",
    "        xmlString = handle.read()\n",
    "        element = xmltodict.parse(xmlString)\n",
    "    \n",
    "        pmc_error = False\n",
    "    \n",
    "        # Check for an error on PMC's side and record it\n",
    "        for key in element['pmc-articleset'].keys():\n",
    "            if key == 'error':\n",
    "                logger.error(f\"PMC API error - ID: {pmcid}\")\n",
    "                pmc_error = True\n",
    "    \n",
    "        if not pmc_error:\n",
    "            with open(file, \"w\") as file_out:\n",
    "                file_out.write(xmlString)\n",
    "        \n",
    "        # This is a delay in accordance with PubMed API usage guidelines\n",
    "        # PubMed allows 3 requests/sec without API key or 10 req/sec with\n",
    "        if time.perf_counter() - start_time < .33:\n",
    "            time.sleep(.33 - (time.perf_counter() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract references from full texts\n",
    "\n",
    "The primary goal here is to extract identifiers (DOI or PMID) for each reference in each article and use these identifiers to created an edge list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessions = []\n",
    "\n",
    "# You might notice the \"nohead\" in the filename, I removed the header for this file\n",
    "# using tail\n",
    "with open(\"../data/2013_MTI_in_OA_train_nohead.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.split(\",\")\n",
    "        accessions.append(line[3])\n",
    "\n",
    "with open(\"../data/2013_MTI_in_OA_test_nohead.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.split(\",\")\n",
    "        accessions.append(line[3])\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(\"reference_extraction.log\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# List for the references\n",
    "mti_refs = []\n",
    "\n",
    "# Extract references from the XML files\n",
    "for ID in tqdm(accessions):\n",
    "    try:\n",
    "        with open(f\"../pmc_xmls/{ID}.xml\", \"r\") as handle:\n",
    "            soup = BeautifulSoup(handle.read())\n",
    "            \n",
    "            sample = [ID]\n",
    "            \n",
    "            # add IDs to the error log if they don't have the 'back' tag and to \n",
    "            # the samples list if they do\n",
    "            if soup.back is None:\n",
    "                logger.error(f\"No refs: {ID}\")\n",
    "            elif soup.back is not None:\n",
    "                for pubid in soup.back.find_all('pub-id'):\n",
    "                    sample.append(pubid.string)\n",
    "                mti_refs.append(sample)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"FNFE: {ID}\")\n",
    "\n",
    "# Create dicts for ID conversions\n",
    "# Here I want to convert my PMCIDs back to PMIDs and convert any DOIs to\n",
    "# PMIDs whenever possible. The DOI -> PMID thing is not extremely important,\n",
    "# because in general PMC is good about adding PMIDs to articles' references \n",
    "# as they become available, but I wanted to be thorough \n",
    "dois = {}\n",
    "pmcids = {}\n",
    "with open(\"../data/PMC-ids-nohead.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.split(\",\")\n",
    "        if len(line) > 9:\n",
    "            if line[7]:\n",
    "                dois[line[7]] = line[9]\n",
    "            pmcids[line[8]] = line[9]\n",
    "\n",
    "# This function converts a DOI or PMCID to a PMID\n",
    "def fetch_pmid(identifier, dois, pmcids, logger):\n",
    "    pmid = \"\"\n",
    "    if re.match(\"^10\\..*$\", identifier):\n",
    "        if identifier in dois.keys():\n",
    "            pmid = dois[identifier]\n",
    "        return pmid if pmid else np.NaN\n",
    "\n",
    "    if re.match(\"^PMC.*$\", identifier) and identifier in pmcids.keys():\n",
    "        pmid = pmcids[identifier]\n",
    "        if pmid:\n",
    "            return pmid\n",
    "        else:\n",
    "            logger.error(f\"PMCID conversion error: {identifier}\")\n",
    "            return identifier\n",
    "    \n",
    "    # Return original identifier if not a DOI or PMCID\n",
    "    return identifier\n",
    "\n",
    "# Convert IDs to PMIDs if possible\n",
    "for sample in mti_refs:\n",
    "    for index in range(len(sample)):\n",
    "        sample[index] = fetch_pmid(sample[index], dois, pmcids, logger)\n",
    "\n",
    "edge_list = []\n",
    "\n",
    "# Convert to edge list format and drop non-PMID identifiers:\n",
    "for sample in mti_refs:\n",
    "    for index in range(1, len(sample)):\n",
    "        if sample[index] is not np.NaN and re.match(\"^\\d*$\", sample[index]):\n",
    "            edge_list.append((sample[0], str(sample[index])))\n",
    "\n",
    "# Remove duplicates:\n",
    "edge_list = list(set(edge_list))\n",
    "edge_list.sort()\n",
    "\n",
    "# Write output\n",
    "with open(\"../data/edge_list.csv\", \"w\") as out:\n",
    "    for edge in edge_list:\n",
    "        out.write(\"\".join([edge[0], \",\", edge[1], \"\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
