{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Term Co-occurrence\n",
    "\n",
    "In the Pubmed corpus, certain pairs of MeSH terms have a significantly higher probability of being applied to a single Pubmed document (citation) than would be expected based on their individual probabilities of being applied to a document. I am hoping that I might be able to use term co-occurrence log-likelihood ratios in order to help inform decision making in my models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Building co-occurrence matrices can be a very memory-intensive task - especially with the number of MeSH terms totalling 29,350 and the current Pubmed corpus containing ~29.1 documents. Determining term co-occurrence in a single step by multiplying the corpus's term-document matrix by its transposed matrix would require tens of terabytes of RAM, so I broke the problem down into more digestable pieces. I used a generator to generate term-document matrices of a fixed size one-at-a-time in order to avoid storing data in memory. I then simply added the co-occurrence matrices with an array in order to keep track of the totals. In order to make things a little easier initially, I decided to first look at co-occurrences only for the top 7,221 most common MeSH terms.\n",
    "\n",
    "### Imports and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import logging\n",
    "import traceback\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_doc_terms(doc_list, term_subset, logger):\n",
    "    doc_terms = {}\n",
    "    doc_pmid = \"\"\n",
    "    term_ids = []\n",
    "\n",
    "    term_set = set(term_subset)\n",
    "\n",
    "    # Compile regexes\n",
    "    pm_article_start = re.compile(r\"\\s*<PubmedArticle>\")\n",
    "    pm_article_stop = re.compile(r\"\\s*</PubmedArticle>\")\n",
    "    pmid = re.compile(r\"\\s*<PMID.*>(\\d*)</PMID>\")\n",
    "    mesh_list_start = re.compile(r\"\\s*<MeshHeadingList>\")\n",
    "    mesh_list_stop = re.compile(r\"\\s*</MeshHeadingList>\")\n",
    "    mesh_term_id = re.compile(r'\\s*<DescriptorName UI=\"(D\\d+)\".*>')\n",
    "\n",
    "    logger.info(\"Starting doc/term counting\")\n",
    "    for doc in tqdm(doc_list):\n",
    "        try:\n",
    "            with open(f\"./pubmed_bulk/{doc}\", \"r\") as handle:\n",
    "                start_doc_count = len(doc_terms.keys())\n",
    "                start_time = time.perf_counter()\n",
    "\n",
    "                line = handle.readline()\n",
    "                while line:\n",
    "                    if pm_article_start.search(line):\n",
    "                        if doc_pmid:\n",
    "                            doc_terms[doc_pmid] = term_ids\n",
    "                            doc_pmid = \"\"\n",
    "                            term_ids = []\n",
    "                        while not pm_article_stop.search(line):\n",
    "                            if not doc_pmid and pmid.search(line):\n",
    "                                doc_pmid = pmid.search(line).group(1)\n",
    "                            if mesh_list_start.search(line):\n",
    "                                while not mesh_list_stop.search(line):\n",
    "                                    mesh_match = mesh_term_id.search(line)\n",
    "                                    if mesh_match and mesh_match.group(1) in term_set:\n",
    "                                        term_ids.append(mesh_match.group(1))\n",
    "                                    line = handle.readline()\n",
    "                            line = handle.readline()\n",
    "                    line = handle.readline()\n",
    "                doc_terms[doc_pmid] = term_ids\n",
    "\n",
    "                # Get count for log\n",
    "                docs_counted = len(doc_terms.keys()) - start_doc_count\n",
    "                # Get elapsed time and truncate for log\n",
    "                elapsed_time = int((time.perf_counter() - start_time) * 10) / 10.0\n",
    "                logger.info(f\"{doc} parsing completed - terms extracted for {docs_counted} documents in {elapsed_time} seconds\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            trace = traceback.format_exc()\n",
    "            logger.error(repr(e))\n",
    "            logger.critical(trace)\n",
    "\n",
    "    logger.info(\"Stopping doc/term counting\")\n",
    "\n",
    "    with open(\"./data/pm_bulk_doc_term_counts.csv\", \"w\") as out:\n",
    "        for doc in doc_terms:\n",
    "            out.write(\"\".join([doc, \",\"]))\n",
    "            out.write(\",\".join(doc_terms[doc]))\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "def td_matrix_gen(file_path, term_subset, docs_per_matrix):\n",
    "    with open(\"./data/pm_bulk_doc_term_counts.csv\", \"r\") as handle:\n",
    "        td_matrix = []\n",
    "        for line in handle:\n",
    "            if len(td_matrix) > docs_per_matrix:\n",
    "                yield td_matrix\n",
    "                td_matrix = []\n",
    "            line = line.strip(\"\\n\").split(\",\")\n",
    "            terms = line[1:]\n",
    "            row = []\n",
    "            for uid in term_subset:\n",
    "                if uid in terms:\n",
    "                    row.append(1)\n",
    "                else:\n",
    "                    row.append(0)\n",
    "            td_matrix.append(row)\n",
    "\n",
    "# A function for multiprocessing\n",
    "def matrix_builder(work_queue, add_queue, id_num):\n",
    "    # Set up logging - I do actually want a logger for each worker\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.FileHandler(f\"./logs/term_co-occurrence_worker{id_num}.log\")\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            matrix = work_queue.get()\n",
    "            if matrix is None:\n",
    "                break\n",
    "            td_matrix = np.array(matrix)\n",
    "            co_matrix = np.dot(td_matrix.transpose(), td_matrix)\n",
    "            add_queue.put(co_matrix)\n",
    "\n",
    "    except Exception as e:\n",
    "        trace = traceback.format_exc()\n",
    "        logger.error(repr(e))\n",
    "        logger.critical(trace)\n",
    "\n",
    "# A function for multiprocessing, pulls from the queue and writes\n",
    "def matrix_adder(add_queue, completed_queue, dim, docs_per_matrix, num, logger):\n",
    "    # Log counts and rates after every n matrices\n",
    "    log_interval = 800\n",
    "    total_processed = 0\n",
    "    co_matrix = np.zeros((dim, dim))\n",
    "    start_time = time.perf_counter()\n",
    "    while True:\n",
    "        if total_processed and total_processed % log_interval == 0:\n",
    "            elapsed_time = int((time.perf_counter() - start_time) * 10) / 10.0\n",
    "            time_per_it = elapsed_time / (docs_per_matrix * log_interval)\n",
    "            logger.info(f\"Adder {num}: {total_processed * docs_per_matrix} docs added to matrix - last batch of \"\n",
    "                        f\"{docs_per_matrix * log_interval} at a rate of {time_per_it} sec/it\")\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "        matrix_to_add = add_queue.get()\n",
    "        if matrix_to_add is None:\n",
    "            completed_queue.put(co_matrix)\n",
    "            break\n",
    "        co_matrix = co_matrix + matrix_to_add\n",
    "        total_processed += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing\n",
    "\n",
    "Please do not try to run this code in this notebook - see the [original code](https://github.com/wigasper/FUSE/blob/master/term_co-occurrence.py) which I am always in the process of making more portable and usable. There is a lot of bloat here related to the multiprocessing architecture, but as I implemented it this way from the start, and never tried a single-threaded approach, I am not going to rewrite the code for single-threaded logic for the sake of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Get command line args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-rc\", \"--recount\", help=\"recount terms for each doc\", type=str)\n",
    "    parser.add_argument(\"-n\", \"--num_docs\", help=\"number of docs to build co-occurrence matrix with\", type=int)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.FileHandler(\"./logs/term_co-occurrence.log\")\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    # Load term subset to count for\n",
    "    term_subset = []\n",
    "    with open(\"./data/subset_terms_list\", \"r\") as handle:\n",
    "        for line in handle:\n",
    "            term_subset.append(line.strip(\"\\n\"))\n",
    "\n",
    "    if args.recount:\n",
    "        docs = os.listdir(\"./pubmed_bulk\")\n",
    "        count_doc_terms(docs, term_subset, logger)\n",
    "\n",
    "    # This value was determined in testing but is kind of arbitrary\n",
    "    # Maybe need to figure out a better way to get this\n",
    "    docs_per_matrix = 34\n",
    "\n",
    "    matrix_gen = td_matrix_gen(\"./data/pm_bulk_doc_term_counts.csv\", term_subset, docs_per_matrix)\n",
    "\n",
    "    # Set up multiprocessing\n",
    "    num_builders = 5\n",
    "    num_adders = 2\n",
    "    add_queue = Queue(maxsize=5)\n",
    "    build_queue = Queue(maxsize=num_builders)\n",
    "    completed_queue = Queue(maxsize=num_adders + 1)\n",
    "\n",
    "    logger.info(f\"Starting workers on {args.num_docs} docs\")\n",
    "\n",
    "    adders = [Process(target=matrix_adder, args=(add_queue, completed_queue, len(term_subset), \n",
    "                    docs_per_matrix, num, logger)) for num in range(num_adders)]\n",
    "    \n",
    "    for adder in adders:\n",
    "        #adder.daemon = True\n",
    "        adder.start()\n",
    "\n",
    "    builders = [Process(target=matrix_builder, args=(build_queue, add_queue, num)) for num in range(num_builders)]\n",
    "\n",
    "    for builder in builders:\n",
    "        builder.start()\n",
    "\n",
    "    if args.num_docs:\n",
    "        limit = args.num_docs / docs_per_matrix\n",
    "    else:\n",
    "        limit = math.inf\n",
    "    \n",
    "    doc_count = 0\n",
    "    for matrix in matrix_gen:\n",
    "        if doc_count < limit:\n",
    "            build_queue.put(matrix)\n",
    "            doc_count += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    while True:\n",
    "        if build_queue.empty():\n",
    "            for _ in range(num_builders):\n",
    "                build_queue.put(None)\n",
    "            break\n",
    "\n",
    "    for builder in builders:\n",
    "        builder.join()\n",
    "\n",
    "    for adder in adders:\n",
    "        add_queue.put(None)\n",
    "\n",
    "    if add_queue.empty():\n",
    "        for adder in adders:\n",
    "            adder.join()\n",
    "\n",
    "    co_matrices = []\n",
    "    for _ in range(num_adders):\n",
    "        co_matrices.append(completed_queue.get())\n",
    "\n",
    "    co_matrix = sum(co_matrices)\n",
    "    \n",
    "    np.save(\"./data/co-occurrence-matrix\", co_matrix)\n",
    "\n",
    "    # Compute probabilities to compare against\n",
    "    term_counts = {}\n",
    "\n",
    "    with open(\"./data/mesh_data.tab\", \"r\") as handle:\n",
    "        for line in handle:\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            term_counts[line[0]] = 0\n",
    "\n",
    "    with open(\"./data/pm_bulk_doc_term_counts.csv\", \"r\") as handle:\n",
    "        for _ in range(doc_count):\n",
    "            line = handle.readline()\n",
    "            line = line.strip(\"\\n\").split(\",\")\n",
    "            terms = line[1:]\n",
    "            terms = [term for term in terms if term]\n",
    "            for term in terms:\n",
    "                term_counts[term] += 1\n",
    "    \n",
    "    # Get probability of each term for the document set\n",
    "    total_terms = sum(term_counts.values())\n",
    "\n",
    "    for term in term_counts:\n",
    "        term_counts[term] = term_counts[term] / total_terms\n",
    "\n",
    "    # Create the expected co-occurrence probability matrix\n",
    "    expected = np.zeros((len(term_subset), len(term_subset)))\n",
    "\n",
    "    for row in range(expected.shape[0]):\n",
    "        for col in range(expected.shape[1]):\n",
    "            expected[row, col] = term_counts[term_subset[row]] * term_counts[term_subset[col]]\n",
    "\n",
    "    # Fill 0s with np.NaN to avoid zero division errors later\n",
    "    expected[expected == 0] = np.NaN\n",
    "\n",
    "    # Get the total number of co-occurrences\n",
    "    total_cooccurrs = 0\n",
    "    for row in range(co_matrix.shape[0]):\n",
    "        for col in range(co_matrix.shape[1]):\n",
    "            if row != col:\n",
    "                total_cooccurrs += co_matrix[row, col]\n",
    "    total_cooccurs = total_cooccurrs / 2\n",
    "\n",
    "    # Create an array just consisting of the total number of co-occurrences in the corpus\n",
    "    temp_total_array = np.full((len(co_matrix), len(co_matrix)), total_cooccurs)\n",
    "\n",
    "    # Divide each number of co-occurrences by the the total to get P(co-occurrence)\n",
    "    co_matrix = np.divide(co_matrix, temp_total_array)\n",
    "\n",
    "    # Divide P(co-occurrence) by expected P(co-occurrence) to get the likelihood ratio\n",
    "    likelihood_ratios = np.divide(co_matrix, expected)\n",
    "\n",
    "    # Set 0s to NaN to avoid taking log(0)\n",
    "    likelihood_ratios[likelihood_ratios == 0] = np.NaN\n",
    "    \n",
    "    # Take the log of the array to get the log-likelihood ratio\n",
    "    log_ratios = np.log(likelihood_ratios)\n",
    "    \n",
    "    with open(\"./data/term_co-occ_log_likelihoods.csv\", \"w\") as out:\n",
    "        for row in range(log_ratios.shape[0]):\n",
    "            for col in range(row + 1, log_ratios.shape[1]):\n",
    "                if not np.isnan(log_ratios[row,col]):\n",
    "                    out.write(\",\".join([term_subset[row], term_subset[col], str(log_ratios[row,col])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "These are some of the top scoring pairs of terms in terms of their log-likelihood ratios - that is, the number of co-occurrences of these pairs of terms was significantly higher than would be expected given their probabilities of occurring individually. It's easy to see how most of the the co-occurrence of many of the top scoring pairs is overrepresented - they are relatively specific terms, so they are unlikely to be in a large amount of documents, and they tend to be closely related. The second most \"enriched\" term pair, \"cerebellar cortex\" and \"population characteristics\", is quite interesting though.\n",
    "\n",
    "The MeSH UID: name dictionary was created by with data obtained by [parsing MeSH](https://github.com/wigasper/FUSE/blob/master/data-aggregation/parse_mesh.py).\n",
    "\n",
    "This data may be affected by using a subset of the Pubmed corpus - these results were generated using 1.9 million documents out of 29 million. In order to have a high log-likelihood ratio, a pair of terms is much more likely to be comprised of very rare terms (because their expected co-occurrance probability is then extremely small). Because of this, it is quite possible that inclusion of a larger portion of the corpus will significantly affect the log-likelihood ratios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History, Early Modern 1451-1600 & History, Modern 1601- - log-likelihood ratio: 9.388012728071354\n",
      "Cerebellar Cortex & Population Characteristics - log-likelihood ratio: 9.166820396888406\n",
      "Sulfamethoxazole & Trimethoprim - log-likelihood ratio: 9.160013526464441\n",
      "Endothelial Growth Factors & Vascular Endothelial Growth Factors - log-likelihood ratio: 8.88587974684798\n",
      "Sevoflurane & Methyl Ethers - log-likelihood ratio: 8.79957650160044\n",
      "Atorvastatin & Heptanoic Acids - log-likelihood ratio: 8.794538707570482\n",
      "Polylactic Acid-Polyglycolic Acid Copolymer & Polyglycolic Acid - log-likelihood ratio: 8.730583630113488\n",
      "Orthodontic Appliances & Orthodontics, Corrective - log-likelihood ratio: 8.68050817100347\n",
      "Hexuronic Acids & Glucuronic Acid - log-likelihood ratio: 8.578496508591751\n",
      "Glycogen Synthase Kinase 3 beta & Glycogen Synthase Kinase 3 - log-likelihood ratio: 8.550509265276437\n",
      "Rotavirus Infections & Rotavirus - log-likelihood ratio: 8.54157487439918\n",
      "Irinotecan & Camptothecin - log-likelihood ratio: 8.541550638731547\n",
      "Clopidogrel & Ticlopidine - log-likelihood ratio: 8.538211737466032\n",
      "Neuromuscular Agents & Botulinum Toxins, Type A - log-likelihood ratio: 8.528572155416391\n",
      "Virulence Factors, Bordetella & Pertussis Toxin - log-likelihood ratio: 8.515648330611059\n",
      "Schizosaccharomyces & Schizosaccharomyces pombe Proteins - log-likelihood ratio: 8.499301033318686\n",
      "Alginates & Hexuronic Acids - log-likelihood ratio: 8.41413461819599\n",
      "Prostaglandins E & Prostaglandins F - log-likelihood ratio: 8.396182263997654\n",
      "Oxaliplatin & Organoplatinum Compounds - log-likelihood ratio: 8.389791732347758\n",
      "Spermidine & Spermine - log-likelihood ratio: 8.358671823633953\n"
     ]
    }
   ],
   "source": [
    "# Load in the log likelihood ratios for term pairs\n",
    "log_likelihood_ratios = []\n",
    "with open(\"./data/term_co-occ_log_likelihoods.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\",\")\n",
    "        line[2] = float(line[2])\n",
    "        log_likelihood_ratios.append(line)\n",
    "\n",
    "# Load in MeSH data to get term names dict\n",
    "term_names = {}\n",
    "\n",
    "with open(\"./data/mesh_data.tab\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        term_names[line[0]] = line[1]\n",
    "            \n",
    "sorted_ratios = sorted(log_likelihood_ratios, key=lambda l:l[2], reverse=True)\n",
    "\n",
    "for ratio in sorted_ratios[0:20]:\n",
    "    print(\"\".join([term_names[ratio[0]], \" & \", term_names[ratio[1]], \" - log-likelihood ratio: \", str(ratio[2])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the lowest scoring pairs are also interesting. These are pairs of terms that would be expected to co-occur at a higher rate than they did in actuality. Unlike the highest scoring pairs, these pairs are generally going to be comprised of frequent terms (which naturally have a higher expected co-occurrence probability) that do not make sense as pairs in the context of research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animals & Managed Care Programs - log-likelihood ratio: -6.139310713749786\n",
      "Animals & Medicare - log-likelihood ratio: -6.022275494165508\n",
      "Middle Aged & Crystallography, X-Ray - log-likelihood ratio: -5.942771983493556\n",
      "United States & Cell Line, Tumor - log-likelihood ratio: -5.703295107437918\n",
      "Animals & Nursing Staff, Hospital - log-likelihood ratio: -5.637691762109699\n",
      "Rats, Inbred Strains & United States - log-likelihood ratio: -5.487749788700135\n",
      "Middle Aged & Gene Expression Regulation, Plant - log-likelihood ratio: -5.4489061847709745\n",
      "Animals & Hospitals, Psychiatric - log-likelihood ratio: -5.394812966673756\n",
      "Adult & Crystallography, X-Ray - log-likelihood ratio: -5.390534872050586\n",
      "Animals & Medical Records Systems, Computerized - log-likelihood ratio: -5.362214038191328\n",
      "Kinetics & Surveys and Questionnaires - log-likelihood ratio: -5.342553058886607\n",
      "Drosophila melanogaster & Middle Aged - log-likelihood ratio: -5.322452029751443\n",
      "Attitude of Health Personnel & Rats - log-likelihood ratio: -5.320241326743496\n",
      "Animals & Education, Nursing - log-likelihood ratio: -5.317439469355249\n",
      "Models, Molecular & Retrospective Studies - log-likelihood ratio: -5.25720839259144\n",
      "Animals & Economic Competition - log-likelihood ratio: -5.252900948217678\n",
      "Hospitals & Rats - log-likelihood ratio: -5.252139170946419\n",
      "Rats, Inbred Strains & Young Adult - log-likelihood ratio: -5.2434202720062615\n",
      "Health Knowledge, Attitudes, Practice & Rats - log-likelihood ratio: -5.226388073982555\n",
      "Animals & Terminal Care - log-likelihood ratio: -5.21317845903084\n"
     ]
    }
   ],
   "source": [
    "sorted_ratios_rev = sorted(log_likelihood_ratios, key=lambda l:l[2])\n",
    "\n",
    "for ratio in sorted_ratios_rev[0:20]:\n",
    "    print(\"\".join([term_names[ratio[0]], \" & \", term_names[ratio[1]], \" - log-likelihood ratio: \", str(ratio[2])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
