{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx_test = []\\nfor doc in test_docs:\\n    row = []\\n    for uid in uids:\\n        if uid in test_freqs[doc].keys():\\n            row.append(test_freqs[doc][uid])\\n        else:\\n            row.append(0)\\n    x_test.append(row)\\n\\nx_test = np.array(x_test)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "subset = []\n",
    "# load in subset terms list\n",
    "with open(\"./data/subset_terms_list\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        subset.append(line.strip(\"\\n\"))\n",
    "subset = set(subset)\n",
    "        \n",
    "with open(\"./data/term_freqs_rev_3_all_terms.json\", \"r\") as handle:\n",
    "    temp = json.load(handle)\n",
    "\n",
    "# Get UIDs in a list - for use in building arrays\n",
    "uids = []\n",
    "with open(\"./data/mesh_data.tab\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        if line[0] in subset:\n",
    "            uids.append(line[0])\n",
    "\n",
    "docs_list = list(temp.keys())\n",
    "partition = int(len(docs_list) * .8)\n",
    "\n",
    "train_docs = docs_list[0:partition]\n",
    "test_docs = docs_list[partition:]\n",
    "\n",
    "test_freqs = {}\n",
    "for doc in test_docs:\n",
    "    test_freqs[doc] = temp[doc]\n",
    "\n",
    "# Load in solution values - only for the docs that we need\n",
    "# Change to set for quick lookup\n",
    "docs_list = set(test_docs)\n",
    "solution = {}\n",
    "with open(\"./data/pm_doc_term_counts.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\",\")\n",
    "        if line[0] in docs_list:\n",
    "            terms = [term for term in line[1:] if term in subset]\n",
    "            if terms:\n",
    "                solution[line[0]] = terms\n",
    "\n",
    "test_docs = [doc for doc in test_docs if doc in solution.keys()]\n",
    "\"\"\"\n",
    "x_test = []\n",
    "for doc in test_docs:\n",
    "    row = []\n",
    "    for uid in uids:\n",
    "        if uid in test_freqs[doc].keys():\n",
    "            row.append(test_freqs[doc][uid])\n",
    "        else:\n",
    "            row.append(0)\n",
    "    x_test.append(row)\n",
    "\n",
    "x_test = np.array(x_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "a = Input(shape=(7221,))\n",
    "b = Dense(2048, activation=\"relu\")(a)\n",
    "b = Dropout(0.1)(b)\n",
    "b = Dense(7221, activation=\"sigmoid\")(b)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "model.load_weights(\"weights.final.7221.hdf5\")\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {doc: [] for doc in test_docs}\n",
    "\n",
    "for r_idx, row in enumerate(y_pred):\n",
    "    for c_idx, col in enumerate(row):\n",
    "        if y_pred[r_idx][c_idx] == 1:\n",
    "            preds[test_docs[r_idx]].append(uids[c_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(preds, solution, uids):\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "\n",
    "    for pmid in preds:\n",
    "        true_pos += len([pred for pred in preds[pmid] if pred in solution[pmid]])\n",
    "        false_pos += len([pred for pred in preds[pmid] if pred not in solution[pmid]])\n",
    "        false_neg += len([sol for sol in solution[pmid] if sol not in preds[pmid]])\n",
    "\n",
    "    if true_pos == 0:\n",
    "        mi_precision = 0\n",
    "        mi_recall = 0\n",
    "        mi_f1 = 0\n",
    "    else:\n",
    "        mi_precision = true_pos / (true_pos + false_pos)\n",
    "        mi_recall = true_pos / (true_pos + false_neg)\n",
    "        mi_f1 = (2 * mi_precision * mi_recall) / (mi_precision + mi_recall)\n",
    "\n",
    "    print(f\"Micro-averaged F1 from test set: {mi_f1}\")\n",
    "    print(f\"Micro-averaged precision from test set: {mi_precision}\")\n",
    "    print(f\"Micro-averaged recall from test set: {mi_recall}\\n\")\n",
    "\n",
    "    eb_ps = []\n",
    "    eb_rs = []\n",
    "    eb_f1s = []\n",
    "\n",
    "    for pmid in preds:\n",
    "        true_pos = len([pred for pred in preds[pmid] if pred in solution[pmid]])\n",
    "        false_pos = len([pred for pred in preds[pmid] if pred not in solution[pmid]])\n",
    "        false_neg = len([sol for sol in solution[pmid] if sol not in preds[pmid]])\n",
    "\n",
    "        if true_pos == 0:\n",
    "            eb_precision = 0\n",
    "            eb_recall = 0\n",
    "            eb_f1 = 0\n",
    "        else:\n",
    "            eb_precision = true_pos / (true_pos + false_pos)\n",
    "            eb_recall = true_pos / (true_pos + false_neg)\n",
    "            eb_f1 = (2 * eb_precision * eb_recall) / (eb_precision + eb_recall)\n",
    "\n",
    "        eb_ps.append(eb_precision)\n",
    "        eb_rs.append(eb_recall)\n",
    "        eb_f1s.append(eb_f1)\n",
    "\n",
    "    eb_f1 = sum(eb_f1s) / len(eb_f1s)\n",
    "    eb_recall = sum(eb_rs) / len(eb_rs)\n",
    "    eb_precision = sum(eb_ps) / len(eb_ps)\n",
    "\n",
    "    print(f\"Example-based F1 from test set: {eb_f1}\")\n",
    "    print(f\"Example-based precision from test set: {eb_precision}\")\n",
    "    print(f\"Example-based recall from test set: {eb_recall}\\n\")\n",
    "\n",
    "    ma_ps = []\n",
    "    ma_rs = []\n",
    "    ma_f1s = []\n",
    "\n",
    "    for uid in uids:\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        false_neg = 0\n",
    "\n",
    "        for pmid in preds:\n",
    "            if uid in preds[pmid] and uid in solution[pmid]:\n",
    "                true_pos += 1\n",
    "            if uid in preds[pmid] and uid not in solution[pmid]:\n",
    "                false_pos += 1\n",
    "            if uid in solution[pmid] and uid not in preds[pmid]:\n",
    "                false_neg += 1\n",
    "\n",
    "        if true_pos == 0:\n",
    "            ma_precision = 0\n",
    "            ma_recall = 0\n",
    "            ma_f1 = 0\n",
    "        else:\n",
    "            ma_precision = true_pos / (true_pos + false_pos)\n",
    "            ma_recall = true_pos / (true_pos + false_neg)\n",
    "            ma_f1 = (2 * ma_precision * ma_recall) / (ma_precision + ma_recall)\n",
    "\n",
    "        if true_pos + false_pos + false_neg > 0:\n",
    "            ma_ps.append(ma_precision)\n",
    "            ma_rs.append(ma_recall)\n",
    "            ma_f1s.append(ma_f1)\n",
    "\n",
    "    ma_f1 = sum(ma_f1s) / len(ma_f1s)\n",
    "    ma_recall = sum(ma_rs) / len(ma_rs)\n",
    "    ma_precision = sum(ma_ps) / len(ma_ps)\n",
    "\n",
    "    print(f\"Macro-averaged F1 from test set: {ma_f1}\")\n",
    "    print(f\"Macro-averaged precision from test set: {ma_precision}\")\n",
    "    print(f\"Macro-averaged recall from test set: {ma_recall}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged F1 from test set: 0.4427023396290225\n",
      "Micro-averaged precision from test set: 0.6937186262531098\n",
      "Micro-averaged recall from test set: 0.3250762247699461\n",
      "\n",
      "Example-based F1 from test set: 0.42814472752355864\n",
      "Example-based precision from test set: 0.6916253773087887\n",
      "Example-based recall from test set: 0.33731962295133666\n",
      "\n",
      "Macro-averaged F1 from test set: 0.16752501915943868\n",
      "Macro-averaged precision from test set: 0.3564535370602334\n",
      "Macro-averaged recall from test set: 0.12394390582064842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, solution, uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, testing sliding threshold model on class subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_freqs, thresh):\n",
    "    # Test it out\n",
    "    predictions = {}\n",
    "\n",
    "    # Predict\n",
    "    for doc in test_freqs.keys():\n",
    "        predictions[doc] = [key for key, val in test_freqs[doc].items() if val > thresh]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged F1 from test set: 0.4827006057041883\n",
      "Micro-averaged precision from test set: 0.48529272319585903\n",
      "Micro-averaged recall from test set: 0.4801360318971345\n",
      "\n",
      "Example-based F1 from test set: 0.4751300711094877\n",
      "Example-based precision from test set: 0.5066948606487902\n",
      "Example-based recall from test set: 0.5006517643847974\n",
      "\n",
      "Macro-averaged F1 from test set: 0.33042238415581837\n",
      "Macro-averaged precision from test set: 0.4377089590998267\n",
      "Macro-averaged recall from test set: 0.31022029978304544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_freqs = {}\n",
    "uids_set = set(uids)\n",
    "for doc in test_docs:\n",
    "    if doc in solution.keys():\n",
    "        test_freqs[doc] = {key: val for key, val in temp[doc].items() if key in uids_set}\n",
    "\n",
    "for pmid in test_freqs:\n",
    "    freqs = test_freqs[pmid]\n",
    "    mean_freq = sum(freqs.values()) / len(freqs.values())\n",
    "    min_freq = min(freqs.values())\n",
    "    max_freq = max(freqs.values())\n",
    "    if max_freq - min_freq > 0:\n",
    "        for freq in freqs:\n",
    "            freqs[freq] = (freqs[freq] - mean_freq) / (max_freq - min_freq)\n",
    "    test_freqs[pmid] = freqs\n",
    "    \n",
    "preds = predict(test_freqs, 0.15)\n",
    "\n",
    "evaluate(preds, solution, uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 500 response NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500 = []\n",
    "with open(\"./data/top_500_terms\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        top_500.append(line.strip(\"\\n\"))\n",
    "top_500_set = set(top_500)        \n",
    "\n",
    "solution = {}\n",
    "with open(\"./data/pm_doc_term_counts.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\",\")\n",
    "        if line[0] in docs_list:\n",
    "            terms = [term for term in line[1:] if term in top_500_set]\n",
    "            solution[line[0]] = terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Input(shape=(7221,))\n",
    "b = Dense(1024, activation=\"relu\")(a)\n",
    "b = Dropout(0.1)(b)\n",
    "b = Dense(500, activation=\"sigmoid\")(b)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "model.load_weights(\"weights.final.500.hdf5\")\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {doc: [] for doc in test_docs}\n",
    "\n",
    "for r_idx, row in enumerate(y_pred):\n",
    "    for c_idx, col in enumerate(row):\n",
    "        if y_pred[r_idx][c_idx] == 1:\n",
    "            preds[test_docs[r_idx]].append(top_500[c_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = top_500\n",
    "\n",
    "#evaluate(preds, solution, uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with variable thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_freqs, thresh):\n",
    "    # Test it out\n",
    "    predictions = {}\n",
    "\n",
    "    # Predict\n",
    "    for doc in test_freqs.keys():\n",
    "        predictions[doc] = [key for key, val in test_freqs[doc].items() if val > thresh]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged F1 from test set: 0.533019874059854\n",
      "Micro-averaged precision from test set: 0.5216558659623518\n",
      "Micro-averaged recall from test set: 0.5448900265453166\n",
      "\n",
      "Example-based F1 from test set: 0.5141958268688661\n",
      "Example-based precision from test set: 0.541053241569883\n",
      "Example-based recall from test set: 0.5743312977299118\n",
      "\n",
      "Macro-averaged F1 from test set: 0.3384002403345578\n",
      "Macro-averaged precision from test set: 0.43785006555819594\n",
      "Macro-averaged recall from test set: 0.3232964353217651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_freqs = {}\n",
    "for doc in test_docs:\n",
    "    if doc in solution.keys():\n",
    "        test_freqs[doc] = {key: val for key, val in temp[doc].items() if key in top_500_set}\n",
    "\n",
    "for pmid in test_freqs:\n",
    "    freqs = test_freqs[pmid]\n",
    "    if len(freqs.values()) > 0:\n",
    "        mean_freq = sum(freqs.values()) / len(freqs.values())\n",
    "        min_freq = min(freqs.values())\n",
    "        max_freq = max(freqs.values())\n",
    "        if max_freq - min_freq > 0:\n",
    "            for freq in freqs:\n",
    "                freqs[freq] = (freqs[freq] - mean_freq) / (max_freq - min_freq)\n",
    "        test_freqs[pmid] = freqs\n",
    "    \n",
    "preds = predict(test_freqs, 0.15)\n",
    "\n",
    "evaluate(preds, solution, uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
