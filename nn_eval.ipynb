{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_test = []\\nfor doc in test_docs:\\n    row = []\\n    for uid in uids:\\n        if uid in solution[doc]:\\n            row.append(1)\\n        else:\\n            row.append(0)\\n    y_test.append(row)\\n\\ny_test = np.array(y_test)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "subset = []\n",
    "# load in subset terms list\n",
    "with open(\"./data/subset_terms_list\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        subset.append(line.strip(\"\\n\"))\n",
    "subset = set(subset)\n",
    "        \n",
    "with open(\"./data/term_freqs_rev_3_all_terms.json\", \"r\") as handle:\n",
    "    temp = json.load(handle)\n",
    "\n",
    "# Get UIDs in a list - for use in building arrays\n",
    "uids = []\n",
    "with open(\"./data/mesh_data.tab\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        if line[0] in subset:\n",
    "            uids.append(line[0])\n",
    "\n",
    "docs_list = list(temp.keys())\n",
    "partition = int(len(docs_list) * .8)\n",
    "\n",
    "train_docs = docs_list[0:partition]\n",
    "test_docs = docs_list[partition:]\n",
    "\n",
    "test_freqs = {}\n",
    "for doc in test_docs:\n",
    "    test_freqs[doc] = temp[doc]\n",
    "\n",
    "# Load in solution values - only for the docs that we need\n",
    "# Change to set for quick lookup\n",
    "docs_list = set(test_docs)\n",
    "solution = {}\n",
    "with open(\"./data/pm_doc_term_counts.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\",\")\n",
    "        if line[0] in docs_list:\n",
    "            terms = [term for term in line[1:] if term in subset]\n",
    "            if terms:\n",
    "                solution[line[0]] = terms\n",
    "\n",
    "test_docs = [doc for doc in test_docs if doc in solution.keys()]\n",
    "\n",
    "x_test = []\n",
    "for doc in test_docs:\n",
    "    row = []\n",
    "    for uid in uids:\n",
    "        if uid in test_freqs[doc].keys():\n",
    "            row.append(test_freqs[doc][uid])\n",
    "        else:\n",
    "            row.append(0)\n",
    "    x_test.append(row)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "\"\"\"\n",
    "y_test = []\n",
    "for doc in test_docs:\n",
    "    row = []\n",
    "    for uid in uids:\n",
    "        if uid in solution[doc]:\n",
    "            row.append(1)\n",
    "        else:\n",
    "            row.append(0)\n",
    "    y_test.append(row)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "a = Input(shape=(7221,))\n",
    "b = Dense(2048, activation=\"relu\")(a)\n",
    "b = Dropout(0.1)(b)\n",
    "b = Dense(7221, activation=\"sigmoid\")(b)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "model.load_weights(\"weights.final.7221.hdf5\")\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff18eec18af849d2ba1dac56900c0d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {doc: [] for doc in test_docs}\n",
    "\n",
    "for r_idx, row in enumerate(y_pred):\n",
    "    for c_idx, col in enumerate(row):\n",
    "        if y_pred[r_idx][c_idx] == 1:\n",
    "            preds[test_docs[r_idx]].append(uids[c_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged F1 from test set: 0.4427023396290225\n",
      "Micro-averaged precision from test set: 0.6937186262531098\n",
      "Micro-averaged recall from test set: 0.3250762247699461\n",
      "\n",
      "Example-based F1 from test set: 0.42814472752355864\n",
      "Example-based precision from test set: 0.6916253773087887\n",
      "Example-based recall from test set: 0.33731962295133666\n",
      "\n",
      "Macro-averaged F1 from test set: 0.16752501915943868\n",
      "Macro-averaged precision from test set: 0.3564535370602334\n",
      "Macro-averaged recall from test set: 0.12394390582064842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_pos = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "\n",
    "for pmid in preds:\n",
    "    true_pos += len([pred for pred in preds[pmid] if pred in solution[pmid]])\n",
    "    false_pos += len([pred for pred in preds[pmid] if pred not in solution[pmid]])\n",
    "    false_neg += len([sol for sol in solution[pmid] if sol not in preds[pmid]])\n",
    "\n",
    "if true_pos == 0:\n",
    "    mi_precision = 0\n",
    "    mi_recall = 0\n",
    "    mi_f1 = 0\n",
    "else:\n",
    "    mi_precision = true_pos / (true_pos + false_pos)\n",
    "    mi_recall = true_pos / (true_pos + false_neg)\n",
    "    mi_f1 = (2 * mi_precision * mi_recall) / (mi_precision + mi_recall)\n",
    "\n",
    "print(f\"Micro-averaged F1 from test set: {mi_f1}\")\n",
    "print(f\"Micro-averaged precision from test set: {mi_precision}\")\n",
    "print(f\"Micro-averaged recall from test set: {mi_recall}\\n\")\n",
    "\n",
    "eb_ps = []\n",
    "eb_rs = []\n",
    "eb_f1s = []\n",
    "\n",
    "for pmid in preds:\n",
    "    true_pos = len([pred for pred in preds[pmid] if pred in solution[pmid]])\n",
    "    false_pos = len([pred for pred in preds[pmid] if pred not in solution[pmid]])\n",
    "    false_neg = len([sol for sol in solution[pmid] if sol not in preds[pmid]])\n",
    "\n",
    "    if true_pos == 0:\n",
    "        eb_precision = 0\n",
    "        eb_recall = 0\n",
    "        eb_f1 = 0\n",
    "    else:\n",
    "        eb_precision = true_pos / (true_pos + false_pos)\n",
    "        eb_recall = true_pos / (true_pos + false_neg)\n",
    "        eb_f1 = (2 * eb_precision * eb_recall) / (eb_precision + eb_recall)\n",
    "\n",
    "    eb_ps.append(eb_precision)\n",
    "    eb_rs.append(eb_recall)\n",
    "    eb_f1s.append(eb_f1)\n",
    "\n",
    "eb_f1 = sum(eb_f1s) / len(eb_f1s)\n",
    "eb_recall = sum(eb_rs) / len(eb_rs)\n",
    "eb_precision = sum(eb_ps) / len(eb_ps)\n",
    "\n",
    "print(f\"Example-based F1 from test set: {eb_f1}\")\n",
    "print(f\"Example-based precision from test set: {eb_precision}\")\n",
    "print(f\"Example-based recall from test set: {eb_recall}\\n\")\n",
    "\n",
    "ma_ps = []\n",
    "ma_rs = []\n",
    "ma_f1s = []\n",
    "\n",
    "for uid in uids:\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for pmid in preds:\n",
    "        if uid in preds[pmid] and uid in solution[pmid]:\n",
    "            true_pos += 1\n",
    "        if uid in preds[pmid] and uid not in solution[pmid]:\n",
    "            false_pos += 1\n",
    "        if uid in solution[pmid] and uid not in preds[pmid]:\n",
    "            false_neg += 1\n",
    "    \n",
    "    if true_pos == 0:\n",
    "        ma_precision = 0\n",
    "        ma_recall = 0\n",
    "        ma_f1 = 0\n",
    "    else:\n",
    "        ma_precision = true_pos / (true_pos + false_pos)\n",
    "        ma_recall = true_pos / (true_pos + false_neg)\n",
    "        ma_f1 = (2 * ma_precision * ma_recall) / (ma_precision + ma_recall)\n",
    "\n",
    "    if true_pos + false_pos + false_neg > 0:\n",
    "        ma_ps.append(ma_precision)\n",
    "        ma_rs.append(ma_recall)\n",
    "        ma_f1s.append(ma_f1)\n",
    "\n",
    "ma_f1 = sum(ma_f1s) / len(ma_f1s)\n",
    "ma_recall = sum(ma_rs) / len(ma_rs)\n",
    "ma_precision = sum(ma_ps) / len(ma_ps)\n",
    "\n",
    "print(f\"Macro-averaged F1 from test set: {ma_f1}\")\n",
    "print(f\"Macro-averaged precision from test set: {ma_precision}\")\n",
    "print(f\"Macro-averaged recall from test set: {ma_recall}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
