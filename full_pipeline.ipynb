{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "from Bio import Entrez\n",
    "from tqdm import tqdm_notebook\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to easily extract references from articles, I first needed to determine which articles are available as full texts in the PMC Open Access subset. To do this, I downloaded the list of all articles in the Open Access subset and determined which PMIDs from the MTI ML Dataset are also in the Open Access subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_list = pd.read_csv(\"oa_file_list.csv\")\n",
    "\n",
    "mti_train = open(\"2013_MTI_ML_DataSet/PMIDs_train\", \"r\")\n",
    "mti_train = mti_train.readlines()\n",
    "mti_train = pd.DataFrame({'PMID':mti_train})\n",
    "\n",
    "mti_test = open(\"2013_MTI_ML_DataSet/PMIDs_test\", \"r\")\n",
    "mti_test = mti_test.readlines()\n",
    "mti_test = pd.DataFrame({'PMID':mti_test})\n",
    "\n",
    "mti_oaSubset_train = oa_list[(oa_list.PMID.isin(mti_train.PMID))]\n",
    "mti_oaSubset_train.to_csv(\"2013_MTI_in_OA_train.csv\", index=False)\n",
    "\n",
    "mti_oaSubset_test = oa_list[(oa_list.PMID.isin(mti_test.PMID))]\n",
    "mti_oaSubset_test.to_csv(\"2013_MTI_in_OA_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I downloaded the full texts for all PMIDs in the 2013 MTI ML Dataset that are also in PMC's Open Access subset (determined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mti_train = pd.read_csv(\"./FUSE/2013_MTI_in_OA_train.csv\")\n",
    "ids_to_get = mti_train[\"Accession ID\"].tolist()\n",
    "\n",
    "# PMC_errors stores any errors from PMC's side\n",
    "pmc_errors = []\n",
    "\n",
    "for ID in tqdm_notebook(ids_to_get):\n",
    "    start_time = time.perf_counter()\n",
    "    file = Path(\"./PMC XMLs/{}.xml\".format(ID))\n",
    "    if not file.exists():\n",
    "        Entrez.email = \"kgasper@unomaha.edu\"\n",
    "        handle = Entrez.efetch(db=\"pmc\", id=ID, retmode=\"xml\")\n",
    "        xmlString = handle.read()\n",
    "        element = xmltodict.parse(xmlString)\n",
    "    \n",
    "        pmc_error = False\n",
    "    \n",
    "        # Check for an error on PMC's side and record it\n",
    "        for key in element['pmc-articleset'].keys():\n",
    "            if key == 'error':\n",
    "                pmc_errors.append(ID)\n",
    "                pmc_error = True\n",
    "    \n",
    "        if not pmc_error:\n",
    "            file_out = open(\"./PMC XMLs/{}.xml\".format(ID), \"w\")\n",
    "            file_out.write(xmlString)\n",
    "            \n",
    "        if time.perf_counter() - start_time < .33:\n",
    "            time.sleep(.33 - (time.perf_counter() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References were extracted from full text XML files using the Beautiful Soup package. Initially, I tried to use xmltodict to do this, which ended up being very difficult. However, I was exposed to all the different formats of the full text XMLs and was able to see that references are always under the \"back\" tag, and they always have the \"pub-id\" identifier in their tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mti_oaSubset_train = pd.read_csv(\"2013_MTI_in_OA_train.csv\")\n",
    "\n",
    "# List for the references\n",
    "mti_refs = [[]]\n",
    "\n",
    "# This list is for IDs that don't have the 'back' tag, to investigate later.\n",
    "ids_to_check = []\n",
    "\n",
    "# FileNotFoundErrors\n",
    "fnfe = []\n",
    "\n",
    "# Extract references from the XML files\n",
    "for ID in tqdm(mti_oaSubset_train['Accession ID']):\n",
    "    try:\n",
    "        handle = open(\"./PMC XMLs/{}.xml\".format(ID), \"r\")\n",
    "        soup = BeautifulSoup(handle.read())\n",
    "        \n",
    "        sample = [ID]\n",
    "        \n",
    "        # add IDs to the error list if they don't have the 'back' tag and to \n",
    "        # the samples list if they do\n",
    "        if soup.back is None:\n",
    "            ids_to_check.append(ID)\n",
    "        elif soup.back is not None:\n",
    "            for pubid in soup.back.find_all('pub-id'):\n",
    "                sample.append(pubid.string)\n",
    "            \n",
    "            mti_refs.append(sample)\n",
    "    except FileNotFoundError:\n",
    "        fnfe.append(ID)\n",
    "    \n",
    "mti_refs = pd.DataFrame(mti_refs)\n",
    "\n",
    "# Read in PMC_IDs to convert all the DOIs to PMIDs:\n",
    "PMC_ids = pd.read_csv(\"PMC-ids.csv\", low_memory=False)\n",
    "\n",
    "# Drop unneeded columns\n",
    "DOI_PMIDs = PMC_ids.drop([\"Journal Title\", \"ISSN\", \"eISSN\", \"Year\", \"Volume\",\n",
    "                         \"Issue\", \"Page\", \"PMCID\", \"Manuscript Id\", \n",
    "                         \"Release Date\"], axis=1)\n",
    "\n",
    "# Change PMIDs from float64 in scientific notation to str\n",
    "DOI_PMIDs.PMID = DOI_PMIDs.PMID.fillna(0)\n",
    "DOI_PMIDs.PMID = DOI_PMIDs.PMID.astype(int).astype(str)\n",
    "DOI_PMIDs.PMID = DOI_PMIDs.PMID.replace(\"0\", \"NA\")\n",
    "\n",
    "# Find DOIs and convert them to PMIDs if possible\n",
    "for row in tqdm(range(0, len(mti_refs))):\n",
    "    for col in range(0, len(mti_refs.columns)):\n",
    "        if re.match(r\"^[1][0][.]..*$\", str(mti_refs.iloc[row, col])):\n",
    "            result = DOI_PMIDs[DOI_PMIDs.DOI == mti_refs.iloc[row, col]].PMID\n",
    "            if len(result) == 1:\n",
    "                 mti_refs.iloc[row, col] = result.item()\n",
    "            if len(result) == 0:\n",
    "                mti_refs.iloc[row, col] = np.NaN\n",
    "            \n",
    "# Remove IDs in the format \"2-s.......\"\n",
    "mti_refs = mti_refs.replace(\"^2[-]s..*$\", np.NaN, regex=True)\n",
    "\n",
    "# Make edge list by melting the DF. Drop unnecessary column and NAs\n",
    "edge_list = pd.melt(mti_refs, id_vars=['0'], \n",
    "                    value_vars=mti_refs.loc[:, \n",
    "                    mti_refs.columns != '0'],\n",
    "                    value_name='1')\n",
    "\n",
    "###### deal with this:\n",
    "edge_list = edge_list.replace(\" 10.1007/s11606-011-1968-2\", \"22282311\")\n",
    "edge_list = edge_list.replace(\"120/4/e902\", \"17908746\")\n",
    "edge_list = edge_list.replace(\"121/3/575\", \"18310208\")\n",
    "edge_list = edge_list.replace(\"353/5/487\", \"16079372\")\n",
    "edge_list = edge_list.replace(\"163/2/141\", \"19188646\")\n",
    "edge_list = edge_list.replace(\"13/7/930\", \"18809644\")\n",
    "######\n",
    "\n",
    "edge_list = edge_list.drop(\"variable\", axis=1)\n",
    "edge_list = edge_list.dropna()\n",
    "\n",
    "# Sort list, drop duplicates and save\n",
    "edge_list = edge_list.sort_values(by=['0'])\n",
    "edge_list = edge_list.drop_duplicates()\n",
    "edge_list.to_csv(\"edge_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
