{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Default threshold function\n",
    "def get_f1(thresh, term_freqs, solution):\n",
    "    # Predict\n",
    "    predictions = {}\n",
    "    for doc in term_freqs.keys():\n",
    "        predictions[doc] = [key for key, val in term_freqs[doc].items() if val > thresh]\n",
    "        \n",
    "    # Get evaluation metrics\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for pmid in predictions:\n",
    "        true_pos += len([pred for pred in predictions[pmid] if pred in solution[pmid]])\n",
    "        false_pos += len([pred for pred in predictions[pmid] if pred not in solution[pmid]])\n",
    "        false_neg += len([sol for sol in solution[pmid] if sol not in predictions[pmid]])\n",
    "\n",
    "    if true_pos == 0:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "    else:\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def train(term_freqs, solution):\n",
    "    curr_thresh = 0.0\n",
    "    step_val = 0.001\n",
    "    f1s = []\n",
    "    \n",
    "    f1s.append(get_f1(curr_thresh, term_freqs, solution))\n",
    "    f1s.append(get_f1(curr_thresh + step_val, term_freqs, solution))\n",
    "    \n",
    "    curr_thresh += step_val\n",
    "    next_thresh_f1 = get_f1(curr_thresh + step_val, term_freqs, solution)\n",
    "    \n",
    "    while not (next_thresh_f1 < f1s[-1] and next_thresh_f1 < f1s[-2] and f1s[-1] < f1s[-2]):\n",
    "        curr_thresh += step_val\n",
    "        f1s.append(get_f1(curr_thresh, term_freqs, solution))\n",
    "        next_thresh_f1 = get_f1(curr_thresh + step_val, term_freqs, solution)\n",
    "    \n",
    "    return curr_thresh - step_val\n",
    "\n",
    "def predict(test_freqs, thresh):\n",
    "    # Test it out\n",
    "    predictions = {}\n",
    "\n",
    "    # Predict\n",
    "    for doc in test_freqs.keys():\n",
    "        mean_freq = sum(test_freqs[doc].values()) / len(test_freqs[doc])\n",
    "        if mean_freq < thresh:\n",
    "            predictions[doc] = [key for key, val in test_freqs[doc].items() if val > thresh]\n",
    "        else:\n",
    "            predictions[doc] = [key for key, val in test_freqs[doc].items() if val > mean_freq]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in term frequencies and partition\n",
    "with open(\"./data/term_freqs_rev_3_all_terms.json\", \"r\") as handle:\n",
    "    temp = json.load(handle)\n",
    "\n",
    "docs_list = list(temp.keys())\n",
    "partition = int(len(docs_list) * .8)\n",
    "\n",
    "train_docs = docs_list[0:partition]\n",
    "test_docs = docs_list[partition:]\n",
    "\n",
    "# Load in solution values\n",
    "solution = {}\n",
    "docs_list = set(docs_list)\n",
    "with open(\"./data/pm_doc_term_counts.csv\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\",\")\n",
    "        if line[0] in docs_list:\n",
    "            # Only use samples indexed with MeSH terms\n",
    "            terms = [term for term in line[1:] if term]\n",
    "            if terms:\n",
    "                solution[line[0]] = terms\n",
    "                \n",
    "# Build training/test data, ensure good solution data is available\n",
    "# Solution data is not always available because documents may not be\n",
    "# indexed - even though obviously some of their references have been indexed\n",
    "train_freqs = {}\n",
    "for doc in train_docs:\n",
    "    if doc in solution.keys():\n",
    "        train_freqs[doc] = temp[doc]\n",
    "\n",
    "test_freqs = {}\n",
    "for doc in test_docs:\n",
    "    if doc in solution.keys():\n",
    "        test_freqs[doc] = temp[doc]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in MeSH data\n",
    "term_names = {}\n",
    "mean_term_depths = {}\n",
    "with open(\"./data/mesh_data.tab\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        term_names[line[0]] = line[1]\n",
    "        mean_depth = 0\n",
    "        posits = [len(posit.split(\".\")) for posit in line[4].split(\",\")]\n",
    "        mean_term_depths[line[0]] = sum(posits) / len(posits)\n",
    "            \n",
    "uids = list(term_names.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = train(train_freqs, solution)\n",
    "print(f\"Learned discrimination threshold: {thresh}\\n\")\n",
    "\n",
    "preds = predict(test_freqs, thresh)\n",
    "\n",
    "true_pos = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "\n",
    "for pmid in preds:\n",
    "    true_pos += len([pred for pred in preds[pmid] if pred in solution[pmid]])\n",
    "    false_pos += len([pred for pred in preds[pmid] if pred not in solution[pmid]])\n",
    "    false_neg += len([sol for sol in solution[pmid] if sol not in preds[pmid]])\n",
    "\n",
    "if true_pos == 0:\n",
    "    mi_precision = 0\n",
    "    mi_recall = 0\n",
    "    mi_f1 = 0\n",
    "else:\n",
    "    mi_precision = true_pos / (true_pos + false_pos)\n",
    "    mi_recall = true_pos / (true_pos + false_neg)\n",
    "    mi_f1 = (2 * mi_precision * mi_recall) / (mi_precision + mi_recall)\n",
    "\n",
    "print(f\"Micro-averaged F1 from test set: {mi_f1}\")\n",
    "print(f\"Micro-averaged precision from test set: {mi_precision}\")\n",
    "print(f\"Micro-averaged recall from test set: {mi_recall}\\n\")\n",
    "\n",
    "ma_ps = []\n",
    "ma_rs = []\n",
    "ma_f1s = []\n",
    "\n",
    "for uid in uids:\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for pmid in preds:\n",
    "        if uid in preds[pmid] and uid in solution[pmid]:\n",
    "            true_pos += 1\n",
    "        if uid in preds[pmid] and uid not in solution[pmid]:\n",
    "            false_pos += 1\n",
    "        if uid in solution[pmid] and uid not in preds[pmid]:\n",
    "            false_neg += 1\n",
    "    \n",
    "    if true_pos == 0:\n",
    "        ma_precision = 0\n",
    "        ma_recall = 0\n",
    "        ma_f1 = 0\n",
    "    else:\n",
    "        ma_precision = true_pos / (true_pos + false_pos)\n",
    "        ma_recall = true_pos / (true_pos + false_neg)\n",
    "        ma_f1 = (2 * ma_precision * ma_recall) / (ma_precision + ma_recall)\n",
    "\n",
    "    if true_pos + false_pos + false_neg > 0:\n",
    "        ma_ps.append(ma_precision)\n",
    "        ma_rs.append(ma_recall)\n",
    "        ma_f1s.append(ma_f1)\n",
    "\n",
    "ma_f1 = sum(ma_f1s) / len(ma_f1s)\n",
    "ma_recall = sum(ma_rs) / len(ma_rs)\n",
    "ma_precision = sum(ma_ps) / len(ma_ps)\n",
    "\n",
    "print(f\"Macro-averaged F1 from test set: {ma_f1}\")\n",
    "print(f\"Macro-averaged precision from test set: {ma_precision}\")\n",
    "print(f\"Macro-averaged recall from test set: {ma_recall}\\n\")\n",
    "\n",
    "eb_ps = []\n",
    "eb_rs = []\n",
    "eb_f1s = []\n",
    "\n",
    "for pmid in preds:\n",
    "    true_pos = len([pred for pred in preds[pmid] if pred in solution[pmid]])\n",
    "    false_pos = len([pred for pred in preds[pmid] if pred not in solution[pmid]])\n",
    "    false_neg = len([sol for sol in solution[pmid] if sol not in preds[pmid]])\n",
    "\n",
    "    if true_pos == 0:\n",
    "        eb_precision = 0\n",
    "        eb_recall = 0\n",
    "        eb_f1 = 0\n",
    "    else:\n",
    "        eb_precision = true_pos / (true_pos + false_pos)\n",
    "        eb_recall = true_pos / (true_pos + false_neg)\n",
    "        eb_f1 = (2 * eb_precision * eb_recall) / (eb_precision + eb_recall)\n",
    "\n",
    "    eb_ps.append(eb_precision)\n",
    "    eb_rs.append(eb_recall)\n",
    "    eb_f1s.append(eb_f1)\n",
    "\n",
    "eb_f1 = sum(eb_f1s) / len(eb_f1s)\n",
    "eb_recall = sum(eb_rs) / len(eb_rs)\n",
    "eb_precision = sum(eb_ps) / len(eb_ps)\n",
    "\n",
    "print(f\"Example-based F1 from test set: {eb_f1}\")\n",
    "print(f\"Example-based precision from test set: {eb_precision}\")\n",
    "print(f\"Example-based recall from test set: {eb_recall}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
