{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Semantic Similarity of MeSH Terms\n",
    "\n",
    "There are, as of May 2019, 29,350 MeSH terms. MeSH terms can be represented on a direct acyclic graph where terms higher on the graph tend to be more ambiguous and have children of increasing specificity. MeSH is frequently referred to as a tree structure, and although it shares many similarities with trees, it is not a tree - most terms occur at multiple places (some dozens of times) on the graph and thus have multiple parent nodes. Because documents are annotated manually, and certainly the huge number of potential terms plays a role in this as well, lots of human biases are infused into the term selection for any given document. Thus, it might be beneficial to have semantic similarity values for all combinations of terms - because different individuals would likely annotate any single document with the same general terms, but might differ when annotating the article with more specific terms. By measuring the semantic similarity of terms, based on the entire Pubmed corpus, I am to capture some of this behavior and potentially incorporate it into my models.\n",
    "\n",
    "I use [Song, Li, Srimani, et al.'s](https://www.ncbi.nlm.nih.gov/pubmed/26356015) method, which was used for measuring the semantic similarity of Gene Ontology terms. This method is based on the graph structure of MeSH and uses aggregatic information content to measure the similarity of any two terms based on the terms' frequencies in the corpus, their shared ancestors' frequencies in the corpus, and their children's frequencies in the corpus.\n",
    "\n",
    "I also ended up implementing multiprocessing to compute semantic similarities in parallel to save time. This architecture ended up requiring me to add quite a bit and I am not sure how functional it would be in a notebook, so please see the [Python code](https://github.com/wigasper/FUSE/blob/master/compute_semantic_similarity.py) that this notebook is based on if you are interested. The calculation logic was unchanged by the multiprocessing implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Gets a list of children for a term. Because we we don't actually have a graph\n",
    "# to traverse, it is done by searching according to the written representation of\n",
    "# its position on the graph\n",
    "def get_children(uid, term_trees):\n",
    "    # Return empty list for terms (like 'D005260' - 'Female') that aren't\n",
    "    # actually part of any trees\n",
    "    if len(term_trees[uid][0]) == 0:\n",
    "        return []\n",
    "    \n",
    "    children = []\n",
    "\n",
    "    for tree in term_trees[uid]:\n",
    "        parent_depth = len(tree.split(\".\"))\n",
    "        for key, vals in term_trees.items():\n",
    "            for val in vals:\n",
    "                child_depth = len(val.split(\".\"))\n",
    "                if tree in val and uid != key and child_depth == parent_depth + 1:\n",
    "                    children.append(key)\n",
    "    \n",
    "    return list(dict.fromkeys(children))\n",
    "\n",
    "# Recursively computes the frequency according to Song et al by adding\n",
    "# the term's count to sum of the counts of all its children\n",
    "def freq(uid, term_counts, term_freqs, term_trees):\n",
    "    total = term_counts[uid]\n",
    "    # Check to see if term has already been computed, avoid recomputation\n",
    "    if term_freqs[uid] != -1:\n",
    "        return term_freqs[uid]\n",
    "    if len(get_children(uid, term_trees)) == 0:\n",
    "        return total\n",
    "    else:\n",
    "        for child in get_children(uid, term_trees):\n",
    "            total += freq(child, term_counts, term_freqs, term_trees)\n",
    "        return total\n",
    "\n",
    "# Get all ancestors of a term\n",
    "def get_ancestors(uid, term_trees, term_trees_rev):\n",
    "    ancestors = [tree for tree in term_trees[uid]]\n",
    "    # Remove empty strings if they exist\n",
    "    ancestors = [ancestor for ancestor in ancestors if ancestor]\n",
    "    idx = 0\n",
    "    while idx < len(ancestors):\n",
    "        ancestors.extend([\".\".join(tree.split(\".\")[:-1]) for tree in term_trees[term_trees_rev[ancestors[idx]]]])\n",
    "        ancestors = [ancestor for ancestor in ancestors if ancestor]\n",
    "        ancestors = list(dict.fromkeys(ancestors))\n",
    "        idx += 1\n",
    "    ancestors = [term_trees_rev[ancestor] for ancestor in ancestors]\n",
    "    ancestors = list(dict.fromkeys(ancestors))\n",
    "    return ancestors\n",
    "\n",
    "# Compute semantic similarity for 2 terms\n",
    "def semantic_similarity(uid1, uid2, sws, svs):\n",
    "    uid1_ancs = get_ancestors(uid1, term_trees, term_trees_rev)\n",
    "    uid2_ancs = get_ancestors(uid2, term_trees, term_trees_rev)\n",
    "    intersection = [anc for anc in uid1_ancs if anc in uid2_ancs]\n",
    "    num = sum([(2 * sws[term]) for term in intersection])\n",
    "    denom = svs[uid1] + svs[uid2]\n",
    "    \n",
    "    return 0 if num is np.NaN or denom is 0 else num / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "When I started this project, I was relatively new to Python. I more or less manually logged everything, but I have since moved on to using the logging module. I can't emphasize enough how valuable this has been. Several times, logging has revealed considerable mistakes that I otherwise would not have noticed. For example, it turns out that occasionally, but not often, BeautifulSoup will truncate very large files without giving any indication to the user - and I only noticed this because of the timestamps in the logs. For this reason, I switched to using regular expressions instead of BeautifulSoup. BeautifulSoup was certainly more elegant and readable than regular expressions, but aside from the data loss issue (the major dealbreaker), it turns out that it is also much slower than regular expressions and uses a magnitude greater memory for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(\"./logs/compute_semantic_similarity.log\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "uids = []\n",
    "names = []\n",
    "trees = []\n",
    "\n",
    "with open(\"./data/mesh_data.tab\", \"r\") as handle:\n",
    "    for line in handle:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        uids.append(line[0])\n",
    "        names.append(line[1])\n",
    "        trees.append(line[4].split(\",\"))\n",
    "\n",
    "docs = os.listdir(\"./pubmed_bulk\")\n",
    "\n",
    "# Create term_trees dict and reverse for quick and easy lookup later\n",
    "term_trees = {uids[idx]:trees[idx] for idx in range(len(uids))}\n",
    "term_trees_rev = {tree:uids[idx] for idx in range(len(uids)) for tree in trees[idx]}\n",
    "\n",
    "term_counts = {uid:0 for uid in uids}\n",
    "\n",
    "# Compile regexes for counting MeSH terms\n",
    "mesh_list_start = re.compile(r\"\\s*<MeshHeadingList>\")\n",
    "mesh_list_stop = re.compile(r\"\\s*</MeshHeadingList>\")\n",
    "mesh_term_id = re.compile(r'\\s*<DescriptorName UI=\"(D\\d+)\".*>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count MeSH terms\n",
    "\n",
    "Next, I count the occurrence of each MeSH term in the entire Pubmed corpus - approximately 29 million documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    try:\n",
    "        with open(\"./pubmed_bulk/{}\".format(doc), \"r\") as handle:\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            line = handle.readline()\n",
    "            while line:\n",
    "                if mesh_list_start.search(line):\n",
    "                    while not mesh_list_stop.search(line):\n",
    "                        if mesh_term_id.search(line):\n",
    "                            term_id = mesh_term_id.search(line).group(1)\n",
    "                            term_counts[term_id] += 1\n",
    "                        line = handle.readline()\n",
    "                line = handle.readline()\n",
    "\n",
    "            # Get elapsed time and truncate for log\n",
    "            elapsed_time = int((time.perf_counter() - start_time) * 10) / 10.0\n",
    "            logger.info(f\"{doc} MeSH term counts completed in {elapsed_time} seconds\")\n",
    "    except Exception as e:\n",
    "        trace = traceback.format_exc()\n",
    "        logger.error(repr(e))\n",
    "        logger.critical(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Semantic Similarity\n",
    "\n",
    "Semantic similarity is computed in a step-by-step process with individual data structures for each step in order to keep things more readable. As previously stated, I use [Song, Li, Srimani, et al.'s](https://www.ncbi.nlm.nih.gov/pubmed/26356015) method here.\n",
    "The process can be done much faster by utilizing multiprocessing, please see the [original code](https://github.com/wigasper/FUSE/blob/master/compute_semantic_similarity.py) that this notebook is based on if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get term frequencies (counts) recursively as described by\n",
    "# Song et al\n",
    "start_time = time.perf_counter()\n",
    "term_freqs = {uid:-1 for uid in uids}\n",
    "for term in term_freqs.keys():\n",
    "    term_freqs[term] = freq(term, term_counts, term_freqs, term_trees)\n",
    "# Get elapsed time and truncate for log\n",
    "elapsed_time = int((time.perf_counter() - start_time) * 10) / 10.0\n",
    "logger.info(f\"Term freqs calculated in {elapsed_time} seconds\")\n",
    "\n",
    "root_freq = sum(term_freqs.values())\n",
    "            \n",
    "# Get term probs\n",
    "term_probs = {uid:np.NaN for uid in uids}\n",
    "for term in term_probs:\n",
    "    term_probs[term] = term_freqs[term] / root_freq\n",
    "\n",
    "# Compute IC values\n",
    "ics = {uid:np.NaN for uid in uids}\n",
    "for term in ics:\n",
    "    try:\n",
    "        ics[term] = -1 * math.log(term_probs[term])\n",
    "    # ZeroDivisionError should not happen with the full corpus\n",
    "    except ZeroDivisionError:\n",
    "        logger.error(f\"ZeroDivisionError for {term}\")\n",
    "\n",
    "# Compute knowledge for each term\n",
    "knowledge = {uid:np.NaN for uid in uids}\n",
    "for term in knowledge:\n",
    "    knowledge[term] = 1 / ics[term]\n",
    "        \n",
    "# Compute semantic weight for each term\n",
    "sws = {uid:np.NaN for uid in uids}\n",
    "for term in sws:\n",
    "    sws[term] = 1 / (1 + math.exp(-1 * knowledge[term]))\n",
    "    \n",
    "# Compute semantic value for each term by adding the semantic weights\n",
    "# of all its ancestors\n",
    "svs = {uid:np.NaN for uid in uids}\n",
    "for term in svs:\n",
    "    sv = 0\n",
    "    ancestors = get_ancestors(term, term_trees, term_trees_rev)\n",
    "    for ancestor in ancestors:\n",
    "        sv += sws[ancestor]\n",
    "    svs[term] = sv\n",
    "\n",
    "# Compute semantic similarity for each pair\n",
    "pairs = {}\n",
    "start_time = time.perf_counter()\n",
    "for pair in combinations(uids, 2):\n",
    "    try:\n",
    "        with open(\"./data/semantic_similarities_rev1.csv\", \"a\") as out:\n",
    "            out.write(\"\".join([pair[0], \",\", pair[1], \",\", str(semantic_similarity(pair[0], pair[1], sws, svs)), \"\\n\"]))\n",
    "    except Exception as e:\n",
    "        trace = traceback.format_exc()\n",
    "        logger.error(repr(e))\n",
    "        logger.critical(trace)\n",
    "\n",
    "# Get elapsed time and truncate for log\n",
    "elapsed_time = int((time.perf_counter() - start_time) * 10) / 10.0\n",
    "logger.info(f\"Semantic similarities calculated in {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some of the most similar terms here ...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
